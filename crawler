import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

# Set to track visited URLs to avoid revisiting
visited = set()

# Function to perform DFS on the website
def dfs_crawl(url, base_url, max_depth=3, depth=0):
    # Stop if the depth limit is reached
    if depth > max_depth:
        return

    # Mark the URL as visited
    visited.add(url)

    try:
        # Fetch the HTML content of the page
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')

        # Find all links on the current page
        for link in soup.find_all('a'):
            href = link.get('href')

            if href:
                # Construct absolute URL
                full_url = urljoin(base_url, href)

                # Only process links within the same domain
                if same_domain(full_url, base_url) and full_url not in visited:
                    print(full_url)
                    # Recursive call to DFS for the new URL
                    dfs_crawl(full_url, base_url, max_depth, depth + 1)

    except requests.exceptions.RequestException as e:
        print(f"Error fetching {url}: {e}")

# Helper function to check if the URL is within the same domain
def same_domain(url, base_url):
    return urlparse(url).netloc == urlparse(base_url).netloc

# URL of the home page (starting point)
home_url = 'https://example.com'

# Start DFS from the home page
dfs_crawl(home_url, home_url, max_depth=3)